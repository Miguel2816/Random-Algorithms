{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SPAM CLASSIFICATION\n",
    "\n",
    "We are going to classify emails by spam or no. We will process the data, tokenize it and it will be the input of our model and the output will be spam or no spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from the nltk corpus of stopwords\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are going to keep the information in the following dict:\n",
    "\n",
    "spam_subject = []\n",
    "spam_text = []\n",
    "\n",
    "ham_subject = []\n",
    "ham_text = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#iterates through each file in enron/spam directory\n",
    "for filename in os.listdir('enron/spam/'):\n",
    "    \n",
    "    #takes out newlines in the file, each line is put into the list 'lines'\n",
    "    lines = [line.rstrip('\\n') for line in open('enron/spam/'+filename, errors='ignore')]\n",
    "    \n",
    "    #lines[0] gets the first line, line[0][9:] \n",
    "    #gets the substring of line[0] starting from index 9.\n",
    "    #lower() makes it all lowercase\n",
    "    tempsubject = lines[0][9:].lower()\n",
    "    \n",
    "    #if the word contains only letters or numbers or both, keep it. Else, toss.\n",
    "    cleaned = [e for e in tempsubject.split(' ') if e.isalnum()]\n",
    "    \n",
    "    #if word is not useless/stopword e.g. 'is', 'the', etc. then keep it, else toss. \n",
    "    #Also toss blanks.\n",
    "    cleaned = [word for word in cleaned if word not in stop_words and word!='']\n",
    "    \n",
    "    #concatenates all elements of the 'cleaned' list with spaces in between the words.\n",
    "    #i.e. makes a sentence.\n",
    "    cleaned = ' '.join(cleaned)\n",
    "    \n",
    "    if len(cleaned)>0:\n",
    "        spam_subject.append(cleaned)\n",
    "    else:\n",
    "        spam_subject.append(' ')#doing this just to have equal length vectors\n",
    "    \n",
    "    temp_spam_text = ' '#placeholder for a single message content\n",
    "    \n",
    "    #so far we have just dealt with the first line - the subject line.\n",
    "    #now we move onto the rest - the message/content/body.\n",
    "    for line in lines[1:]:\n",
    "        temptext = line.lower()\n",
    "        cleaned = [e for e in temptext.split(' ') if e.isalnum()]\n",
    "        cleaned = [word for word in cleaned if word not in stop_words and word!='']\n",
    "        cleaned = ' '.join(cleaned)\n",
    "        if len(cleaned)>0:\n",
    "            #put the whole message together by appending each cleaned line to the previous ones\n",
    "            temp_spam_text+=cleaned + ' '\n",
    "    #finally put the cleaned entire message with no blank spaces at the ends into\n",
    "    #the spam_text list. This is the final version of the cleaned message for this file.\n",
    "    spam_text.append(temp_spam_text.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in os.listdir('enron/ham/'):\n",
    "    lines = [line.rstrip('\\n') for line in open('enron/ham/'+filename)]\n",
    "    tempsubject = lines[0][9:].lower()\n",
    "    cleaned = [e for e in tempsubject.split(' ') if e.isalnum()]\n",
    "    cleaned = [word for word in cleaned if word not in stop_words and word!='']\n",
    "    cleaned = ' '.join(cleaned) # Le quitamos el formato diccionario\n",
    "    if len(cleaned)>0:\n",
    "        ham_subject.append(cleaned)\n",
    "    else:\n",
    "        ham_subject.append(' ')    \n",
    "    temp_ham_text = ' '\n",
    "    for line in lines[1:]:\n",
    "        temptext = line.lower()\n",
    "        cleaned = [e for e in temptext.split(' ') if e.isalnum()]\n",
    "        cleaned = [word for word in cleaned if word not in stop_words and word!='']\n",
    "        cleaned = ' '.join(cleaned)\n",
    "        if len(cleaned)>0:\n",
    "            temp_ham_text+=cleaned+' '\n",
    "    ham_text.append(temp_ham_text.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16545\n",
      "16545\n",
      "17157\n",
      "17157\n"
     ]
    }
   ],
   "source": [
    "print (len(ham_subject))\n",
    "print (len(ham_text))\n",
    "print (len(spam_subject))\n",
    "print (len(spam_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I thought this was more useful instead of separate subject/message\n",
    "#thought of this a bit late so I kept the previous code anyway\n",
    "#basically put the subject and message together into the ham_subj_text list.\n",
    "ham_subj_text = []\n",
    "for i in range(len(ham_subject)):\n",
    "    ham_subj_text.append(ham_subject[i]+' '+ham_text[i])\n",
    "    \n",
    "spam_subj_text = []\n",
    "for i in range(len(spam_subject)):\n",
    "    spam_subj_text.append(spam_subject[i]+' '+spam_text[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33702\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>text</th>\n",
       "      <th>subj_text</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>christmas tree farm pictures</td>\n",
       "      <td></td>\n",
       "      <td>christmas tree farm pictures</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rankings</td>\n",
       "      <td>thank</td>\n",
       "      <td>rankings thank</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>leadership development pilot</td>\n",
       "      <td>sally timing ask shall receive per discussion ...</td>\n",
       "      <td>leadership development pilot sally timing ask ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>key dates impact upcoming sap implementation</td>\n",
       "      <td>next weeks project apollo beyond conduct final...</td>\n",
       "      <td>key dates impact upcoming sap implementation n...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>key hr issues going forward</td>\n",
       "      <td>year end reviews report needs generating like ...</td>\n",
       "      <td>key hr issues going forward year end reviews r...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        subject  \\\n",
       "0                  christmas tree farm pictures   \n",
       "1                                      rankings   \n",
       "2                  leadership development pilot   \n",
       "3  key dates impact upcoming sap implementation   \n",
       "4                   key hr issues going forward   \n",
       "\n",
       "                                                text  \\\n",
       "0                                                      \n",
       "1                                              thank   \n",
       "2  sally timing ask shall receive per discussion ...   \n",
       "3  next weeks project apollo beyond conduct final...   \n",
       "4  year end reviews report needs generating like ...   \n",
       "\n",
       "                                           subj_text  class  \n",
       "0                      christmas tree farm pictures       1  \n",
       "1                                     rankings thank      1  \n",
       "2  leadership development pilot sally timing ask ...      1  \n",
       "3  key dates impact upcoming sap implementation n...      1  \n",
       "4  key hr issues going forward year end reviews r...      1  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create a new pandas dataframe with columns 'subject', 'text', 'subj_text', 'class'\n",
    "#class is the vector which contains the label for each message i.e. ham/spam\n",
    "#class is encoded as 1 for ham, 0 for spam\n",
    "data = pd.DataFrame({'subject':ham_subject+spam_subject,\n",
    "                      'text':ham_text+spam_text,\n",
    "                      'subj_text':ham_subj_text+spam_subj_text,\n",
    "                      'class':[1]*len(ham_subject)+[0]*len(spam_subject)})\n",
    "print (len(data))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 35159)\t1\n",
      "  (0, 57583)\t1\n",
      "  (0, 108757)\t1\n",
      "  (0, 140599)\t1\n",
      "  (1, 116621)\t1\n",
      "  (1, 137410)\t1\n",
      "  (2, 288)\t2\n",
      "  (2, 479)\t1\n",
      "  (2, 720)\t1\n",
      "  (2, 1053)\t2\n",
      "  (2, 1625)\t3\n",
      "  (2, 2609)\t2\n",
      "  (2, 3048)\t2\n",
      "  (2, 4177)\t1\n",
      "  (2, 6044)\t1\n",
      "  (2, 7499)\t1\n",
      "  (2, 8975)\t1\n",
      "  (2, 11029)\t1\n",
      "  (2, 17718)\t1\n",
      "  (2, 18010)\t1\n",
      "  (2, 20349)\t1\n",
      "  (2, 21127)\t2\n",
      "  (2, 21618)\t1\n",
      "  (2, 21631)\t1\n",
      "  (2, 22279)\t1\n",
      "  :\t:\n",
      "  (33701, 106632)\t2\n",
      "  (33701, 106757)\t1\n",
      "  (33701, 108554)\t1\n",
      "  (33701, 108593)\t1\n",
      "  (33701, 109094)\t1\n",
      "  (33701, 109767)\t1\n",
      "  (33701, 112973)\t1\n",
      "  (33701, 114377)\t1\n",
      "  (33701, 118884)\t1\n",
      "  (33701, 120284)\t1\n",
      "  (33701, 122035)\t1\n",
      "  (33701, 124295)\t1\n",
      "  (33701, 129032)\t1\n",
      "  (33701, 132966)\t1\n",
      "  (33701, 135576)\t1\n",
      "  (33701, 138446)\t1\n",
      "  (33701, 144417)\t1\n",
      "  (33701, 148894)\t1\n",
      "  (33701, 149110)\t1\n",
      "  (33701, 149657)\t1\n",
      "  (33701, 149709)\t1\n",
      "  (33701, 149938)\t1\n",
      "  (33701, 152152)\t1\n",
      "  (33701, 152547)\t3\n",
      "  (33701, 156513)\t1\n"
     ]
    }
   ],
   "source": [
    "#epic feature engineering with countvectorizer - simply put, it counts the # of whatever word\n",
    "#was in your message, and puts it in the appropriate unique column.\n",
    "#I believe it's one column per word if I recall correctly. \n",
    "#VERY sparse matrix. \n",
    "count_vectorizer = CountVectorizer()\n",
    "\n",
    "#creates this matrix that I was writing about above\n",
    "count_vectorizer.fit(data['subj_text'].values)\n",
    "counts = count_vectorizer.transform(data['subj_text'].values)\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\0015658\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:   0.9800902023618777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\0015658\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:  0.9801551845879587\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\0015658\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall:     0.9800902023618777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\0015658\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1:         0.9800881013694407\n"
     ]
    }
   ],
   "source": [
    "#start making a model\n",
    "clf = LogisticRegression()\n",
    "\n",
    "#3-fold cross-validation of the model, 5-fold is more often used but if 3-fold performs well,\n",
    "#then your model is golden.\n",
    "#clf,counts,data['class'] is just the model,data/matrix,class-labels for each row in the matrix\n",
    "#cv=3 is the number of folds in k-fold cross-validation (cross-validation = cv)\n",
    "scores = cross_val_score(clf, counts, data['class'], cv=3)\n",
    "\n",
    "#print out the average of the 3 values from the 3-fold cross-validation\n",
    "print ('Accuracy:  ', np.mean(scores))\n",
    "\n",
    "#can make precision/recall/f1/etc. with different scoring\n",
    "precisions = cross_val_score(clf, counts, data['class'], cv=3, scoring='precision_weighted')\n",
    "print ('Precision: ', np.mean(precisions))\n",
    "recalls = cross_val_score(clf, counts, data['class'], cv=3, scoring='recall_weighted')\n",
    "print ('Recall:    ', np.mean(recalls))\n",
    "f1s = cross_val_score(clf, counts, data['class'], cv=3, scoring='f1_weighted')\n",
    "print ('F1:        ', np.mean(f1s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9.99506011e-01 4.93989090e-04]]\n",
      "[0]\n"
     ]
    }
   ],
   "source": [
    "# EXAMPLE\n",
    "\n",
    "line = '''At the moment our team is looking for a manager in your area. We are\n",
    "looking for somebody who is ready to learn and start immediately. After\n",
    "reviewing your CV, we came to the conclusion that you are an ideal\n",
    "candidate for this job position.\n",
    "\n",
    "Our company is engaged in providing services in the area of health\n",
    "insurance. During our work, we have helped thousands of people around the\n",
    "world and we earned an irrefutable reputation. Now you have the opportunity\n",
    "to become a part of our friendly team.\n",
    "\n",
    "Position requirements:\n",
    "- You must be a US citizen.\n",
    "- Your age must be more than 21 years.\n",
    "- You must have a stable internet connection.\n",
    "- You must be willing to learn and improve.\n",
    "\n",
    "Position responsibilities:\n",
    "- Keeping your projects documentation and filling of reports.\n",
    "- Providing quality service to clients of the company.\n",
    "- Perform the tasks on time.\n",
    "- Close cooperation with other managers and our experts.\n",
    "\n",
    "Your salary will be 3000 $ per month plus 500 $ every week. Also, you will\n",
    "have the personal bonuses. If you are ready to start working, respond to\n",
    "this email. We will give you a trial period after which you can decide this\n",
    "job is right for you or not. Hope to hear from you soon.\n",
    "\n",
    "Best regards,\n",
    "Orli Irwin.'''\n",
    "\n",
    "#gotta clean it the same way I did with the training examples for it to work properly\n",
    "temptext = line.lower()\n",
    "cleaned = [e for e in temptext.split(' ') if e.isalnum()]\n",
    "cleaned = [word for word in cleaned if word not in stop_words and word!='']\n",
    "cleaned = ' '.join(cleaned)\n",
    "transformed = count_vectorizer.transform([cleaned])\n",
    "\n",
    "#make the model, train the model, make a prediction.\n",
    "clf = LogisticRegression()\n",
    "clf.fit(counts,data['class'])\n",
    "\n",
    "#probabilities for choosing a class. first in the array is 0's prob, next is 1's prob.\n",
    "#picks the one with the highest prob. \n",
    "print (clf.predict_proba(transformed))\n",
    "\n",
    "#spits out the highest prob prediction.\n",
    "print (clf.predict(transformed))\n",
    "\n",
    "#tada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
